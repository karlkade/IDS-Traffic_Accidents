{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73c58f0",
   "metadata": {},
   "source": [
    "Below are all the required packages for this notebook. Uncomment any packages you are missing and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417a534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install numpy\n",
    "#!pip install matplotlib\n",
    "#!pip install seaborn\n",
    "#!pip install pyproj\n",
    "#!pip install folium\n",
    "#!pip install scikit-learn\n",
    "#!pip install imbalanced-learn\n",
    "#!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d0b550-5b56-4c17-9ab6-b51a876c8294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# set display option to show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# load the dataset\n",
    "df = pd.read_csv(\"traffic.csv\", header=None)\n",
    "\n",
    "# assign column names\n",
    "column_names = [\n",
    "    \"Juhtumi nr\", \"Toimumisaeg\", \"Isikuid\", \"Hukkunuid\", \"Vigastatuid\", \n",
    "    \"Sõidukeid\", \"Aadress (PPA)\", \"Maja nr (PPA)\", \"Tänav (PPA)\", \n",
    "    \"Ristuv tänav (PPA)\", \"Maakond (PPA)\", \"Omavalitsus (PPA)\", \n",
    "    \"Asustus (PPA)\", \"Asula\", \"Liiklusõnnetuse liik [1]\", \n",
    "    \"Liiklusõnnetuse liik [3]\", \"Kergliikurijuhi osalusel\", \n",
    "    \"Jalakäija osalusel\", \"Kaassõitja osalusel\", \"Maastikusõiduki juhi osalusel\",\n",
    "    \"Eaka (65+) mootorsõidukijuhi osalusel\", \"Bussijuhi osalusel\", \n",
    "    \"Veoautojuhi osalusel\", \"Ühissõidukijuhi osalusel\", \"Sõiduautojuhi osalusel\",\n",
    "    \"Mootorratturi osalusel\", \"Mopeedijuhi osalusel\", \"Jalgratturi osalusel\", \n",
    "    \"Alaealise osalusel\", \"Turvavarustust mitte kasutanud isiku osalusel\", \n",
    "    \"Esmase juhiloa omaniku osalusel\", \"Mootorsõidukijuhi osalusel\", \n",
    "    \"Tüüpskeemi nr\", \"Tüüpskeem [2]\", \"Tee tüüp [1]\", \"Tee tüüp [2]\", \n",
    "    \"Tee element [1]\", \"Tee element [2]\", \"Tee objekt [2]\", \n",
    "    \"Kurvilisus\", \"Tee tasasus\", \"Tee seisund\", \"Teekate\", \n",
    "    \"Teekatte seisund [2]\", \"Sõiduradade arv\", \"Lubatud sõidukiirus (PPA)\", \n",
    "    \"Tee nr (PPA)\", \"Tee km (PPA)\", \"Ilmastik [1]\", \"Valgustus [1]\", \n",
    "    \"Valgustus [2]\", \"GPS X\", \"GPS Y\"\n",
    "]\n",
    "\n",
    "df.columns = column_names\n",
    "\n",
    "df[\"GPS X\"] = pd.to_numeric(df[\"GPS X\"], errors=\"coerce\")\n",
    "df[\"GPS Y\"] = pd.to_numeric(df[\"GPS Y\"], errors=\"coerce\")\n",
    "\n",
    "# function to check if time component is present\n",
    "def time_present(date_str):\n",
    "    if isinstance(date_str, str) and re.search(r'\\d{1,2}[:]\\d{2}', date_str):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# apply the function to check for time presence\n",
    "df['Time_Present'] = df['Toimumisaeg'].apply(time_present)\n",
    "\n",
    "# parse the 'Toimumisaeg' column\n",
    "df['Parsed_Date'] = pd.to_datetime(df['Toimumisaeg'], dayfirst=True, errors='coerce')\n",
    "\n",
    "# extract date features\n",
    "df['Year'] = df['Parsed_Date'].dt.year\n",
    "df['Month'] = df['Parsed_Date'].dt.month\n",
    "df['DayOfWeek'] = df['Parsed_Date'].dt.dayofweek  # 0 = monday\n",
    "\n",
    "# set 'Hour' to -1 if time is missing\n",
    "df['Hour'] = df['Parsed_Date'].dt.hour\n",
    "df.loc[~df['Time_Present'], 'Hour'] = -1\n",
    "\n",
    "# handle missing time-based features\n",
    "time_columns = ['Year', 'Month', 'DayOfWeek', 'Hour']\n",
    "df[time_columns] = df[time_columns].fillna(-1).astype(int)\n",
    "\n",
    "# clean numerical columns\n",
    "numerical_columns = ['Isikuid', 'Hukkunuid', 'Vigastatuid', 'Sõidukeid', 'Sõiduradade arv']\n",
    "df[numerical_columns] = df[numerical_columns].apply(pd.to_numeric, errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# create binary columns for fatalities and injuries\n",
    "df['Fatal_Accident'] = (df['Hukkunuid'] > 0).astype(int)\n",
    "df['Injury_Accident'] = (df['Vigastatuid'] > 0).astype(int)\n",
    "\n",
    "# standardize categorical variables (strip whitespace and convert to lowercase)\n",
    "categorical_columns = [\n",
    "    'Maakond (PPA)', 'Ilmastik [1]', 'Tee tüüp [1]', \n",
    "    'Valgustus [1]', 'Tee seisund', 'Teekatte seisund [2]'\n",
    "]\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "\n",
    "# display the first few rows to verify\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9267f18-c485-44d6-8e7d-368292b53df9",
   "metadata": {},
   "source": [
    "# Task 2: Identifying High-Risk Locations and Conditions\n",
    "In this section, we'll analyze the dataset to identify locations and conditions that are associated with higher accident counts and fatality rates.\n",
    "\n",
    "#### Task 2a: High-Risk Locations\n",
    "We'll identify the locations with the highest number of accidents and fatalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201dd0ca-14e2-4571-9d82-73fe6bb27a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# filter out entries with missing 'Maakond (PPA)'\n",
    "df_valid_locations = df[df['Maakond (PPA)'].notna()]\n",
    "\n",
    "# accidents by county\n",
    "accidents_by_county = df_valid_locations['Maakond (PPA)'].value_counts().reset_index()\n",
    "accidents_by_county.columns = ['Maakond (PPA)', 'Accident_Count']\n",
    "\n",
    "# plot accidents by county\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=accidents_by_county, x='Maakond (PPA)', y='Accident_Count')\n",
    "plt.title('Accidents by County')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# fatal accidents by county\n",
    "fatal_accidents_by_county = df_valid_locations[df_valid_locations['Fatal_Accident'] == 1]['Maakond (PPA)'].value_counts().reset_index()\n",
    "fatal_accidents_by_county.columns = ['Maakond (PPA)', 'Fatal_Accident_Count']\n",
    "\n",
    "# plot fatal accidents by county\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=fatal_accidents_by_county, x='Maakond (PPA)', y='Fatal_Accident_Count', color='red')\n",
    "plt.title('Fatal Accidents by County')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd347ad1-8bbd-45d4-a1a3-0df2f41f71f5",
   "metadata": {},
   "source": [
    "## Task 2b: High-Risk Conditions\n",
    "#### We'll analyze environmental and road conditions to see which are most associated with accidents and fatalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa5608-5e45-4a5e-a011-00699933b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define conditions to analyze\n",
    "conditions = ['Ilmastik [1]', 'Tee tüüp [1]', 'Valgustus [1]', 'Tee seisund', 'Teekatte seisund [2]']\n",
    "\n",
    "# calculate fatality risk for each condition\n",
    "condition_risks = []\n",
    "\n",
    "for condition in conditions:\n",
    "    # filter out entries with missing condition data\n",
    "    df_condition = df[df[condition].notna()]\n",
    "    \n",
    "    # create a crosstab of accidents and fatalities\n",
    "    crosstab = pd.crosstab(df_condition[condition], df_condition['Fatal_Accident'])\n",
    "    crosstab['Fatality_Risk'] = crosstab[1] / (crosstab[0] + crosstab[1])\n",
    "    crosstab = crosstab.reset_index()\n",
    "    crosstab['Condition'] = condition\n",
    "\n",
    "    condition_risks.append(crosstab)\n",
    "\n",
    "# combine all condition risks\n",
    "condition_risks_df = pd.concat(condition_risks)\n",
    "\n",
    "# visualize fatality risk by condition\n",
    "for condition in conditions:\n",
    "    subset = condition_risks_df[condition_risks_df['Condition'] == condition]\n",
    "    subset = subset.sort_values('Fatality_Risk', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(14, 8), constrained_layout=True)  # Larger figure with constrained layout\n",
    "    sns.barplot(data=subset, x=condition, y='Fatality_Risk')\n",
    "    plt.title(f'Fatality Risk by {condition}')\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate labels for better fit\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136fba35-ce55-49d2-8935-184106cf4289",
   "metadata": {},
   "source": [
    "Task 2c: Time-Based Analysis\n",
    "We'll examine how accidents and fatalities vary by time of day, day of the week, and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c77c0-a13a-4dd2-b1d1-ea04cb9d72a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out entries with invalid month\n",
    "df_valid_time = df[df['Month'] != -1]\n",
    "\n",
    "# accidents by hour\n",
    "hourly_accidents = df_valid_time[df_valid_time['Hour'] != -1].groupby('Hour')['Juhtumi nr'].count().reset_index(name='Accident_Count')\n",
    "hourly_fatal_accidents = df_valid_time[(df_valid_time['Fatal_Accident'] == 1) & (df_valid_time['Hour'] != -1)].groupby('Hour')['Juhtumi nr'].count().reset_index(name='Fatal_Accident_Count')\n",
    "\n",
    "hourly_data = pd.merge(hourly_accidents, hourly_fatal_accidents, on='Hour', how='left').fillna(0)\n",
    "hourly_data['Fatality_Risk'] = hourly_data['Fatal_Accident_Count'] / hourly_data['Accident_Count']\n",
    "\n",
    "# plot accidents by hour\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=hourly_data, x='Hour', y='Accident_Count', label='Total Accidents')\n",
    "sns.lineplot(data=hourly_data, x='Hour', y='Fatal_Accident_Count', label='Fatal Accidents', color='red')\n",
    "plt.title('Accidents by Hour')\n",
    "plt.xticks(range(0, 24))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plot fatality risk by hour\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=hourly_data, x='Hour', y='Fatality_Risk')\n",
    "plt.title('Fatality Risk by Hour')\n",
    "plt.xticks(range(0, 24))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# accidents by day of the week\n",
    "day_labels = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "df_valid_time = df_valid_time[df_valid_time['DayOfWeek'] != -1]\n",
    "df_valid_time['DayOfWeek_Label'] = df_valid_time['DayOfWeek'].apply(lambda x: day_labels[x] if x >= 0 and x < 7 else 'Unknown')\n",
    "\n",
    "day_accidents = df_valid_time.groupby('DayOfWeek_Label')['Juhtumi nr'].count().reset_index(name='Accident_Count')\n",
    "day_fatal_accidents = df_valid_time[df_valid_time['Fatal_Accident'] == 1].groupby('DayOfWeek_Label')['Juhtumi nr'].count().reset_index(name='Fatal_Accident_Count')\n",
    "\n",
    "day_data = pd.merge(day_accidents, day_fatal_accidents, on='DayOfWeek_Label', how='left').fillna(0)\n",
    "day_data['Fatality_Risk'] = day_data['Fatal_Accident_Count'] / day_data['Accident_Count']\n",
    "\n",
    "# plot accidents by day of the week\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=day_data, x='DayOfWeek_Label', y='Accident_Count')\n",
    "plt.title('Accidents by Day of the Week')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plot fatality risk by day of the week\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=day_data, x='DayOfWeek_Label', y='Fatality_Risk')\n",
    "plt.title('Fatality Risk by Day of the Week')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# accidents by month\n",
    "monthly_accidents = df_valid_time.groupby('Month')['Juhtumi nr'].count().reset_index(name='Accident_Count')\n",
    "monthly_fatal_accidents = df_valid_time[df_valid_time['Fatal_Accident'] == 1].groupby('Month')['Juhtumi nr'].count().reset_index(name='Fatal_Accident_Count')\n",
    "\n",
    "monthly_data = pd.merge(monthly_accidents, monthly_fatal_accidents, on='Month', how='left').fillna(0)\n",
    "monthly_data['Fatality_Risk'] = monthly_data['Fatal_Accident_Count'] / monthly_data['Accident_Count']\n",
    "\n",
    "# exclude Month == -1 (already done above)\n",
    "\n",
    "# plot accidents by month\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=monthly_data, x='Month', y='Accident_Count')\n",
    "plt.title('Accidents by Month')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plot fatality risk by month\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=monthly_data, x='Month', y='Fatality_Risk')\n",
    "plt.title('Fatality Risk by Month')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f874b8-24e3-4c47-a26f-e36a34dabc25",
   "metadata": {},
   "source": [
    "## Task 2d: Driver Profiling\n",
    "#### We'll analyze driver-related factors to identify profiles associated with higher accident involvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d71a0-1acf-4b76-b161-d08b19470c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver-related columns\n",
    "driver_columns = [\n",
    "    'Esmase juhiloa omaniku osalusel', \n",
    "    'Eaka (65+) mootorsõidukijuhi osalusel', \n",
    "    'Alaealise osalusel'\n",
    "]\n",
    "\n",
    "# ensure driver columns are numeric\n",
    "for col in driver_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# calculate fatality risk for each driver category\n",
    "driver_risks = []\n",
    "\n",
    "for col in driver_columns:\n",
    "    total_accidents = df[col].sum()\n",
    "    fatal_accidents = df[df['Fatal_Accident'] == 1][col].sum()\n",
    "    fatality_risk = fatal_accidents / total_accidents if total_accidents > 0 else 0\n",
    "    driver_risks.append({\n",
    "        'Driver_Category': col,\n",
    "        'Total_Accidents': total_accidents,\n",
    "        'Fatal_Accidents': fatal_accidents,\n",
    "        'Fatality_Risk': fatality_risk\n",
    "    })\n",
    "\n",
    "driver_risks_df = pd.DataFrame(driver_risks)\n",
    "\n",
    "# visualize fatality risk by driver category\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=driver_risks_df, x='Driver_Category', y='Fatality_Risk')\n",
    "plt.title('Fatality Risk by Driver Category')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d23e9e9-2f58-429b-aa2e-8058de90faa2",
   "metadata": {},
   "source": [
    "# Task 2e: High-Risk Streets and Spatial Analysis\n",
    "In this section, we'll identify the streets with the highest number of accidents and fatalities.\n",
    "\n",
    "# We'll then visualize these locations using GPS coordinates and create heatmap for Estonia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c31e185-5d0e-495d-902c-16781cd0c091",
   "metadata": {},
   "source": [
    "Top 10 Streets with Highest Number of Accidents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254a1d19-5aac-421a-ae0b-8a40c68c6528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Tänav (PPA)' is standardized\n",
    "df['Tänav (PPA)'] = df['Tänav (PPA)'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Count accidents by street\n",
    "accidents_by_street = df['Tänav (PPA)'].value_counts().reset_index()\n",
    "accidents_by_street.columns = ['Street', 'Accident_Count']\n",
    "\n",
    "# Get top 10 streets\n",
    "top10_accident_streets = accidents_by_street.iloc[1:11]\n",
    "print(\"Top 10 Streets with Highest Number of Accidents:\")\n",
    "print(top10_accident_streets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf34bcf-a22e-4adb-b509-81d4144eff96",
   "metadata": {},
   "source": [
    "Top 10 Streets with Highest Number of Fatalities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea70aa7-8d74-434e-82d2-2a34a9539b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for fatal accidents\n",
    "fatal_df = df[df['Fatal_Accident'] == 1]\n",
    "\n",
    "# Count fatal accidents by street\n",
    "fatalities_by_street = fatal_df['Tänav (PPA)'].value_counts().reset_index()\n",
    "fatalities_by_street.columns = ['Street', 'Fatal_Accident_Count']\n",
    "\n",
    "# Get top 10 streets\n",
    "top10_fatal_streets = fatalities_by_street.iloc[1:11]\n",
    "print(\"\\nTop 10 Streets with Highest Number of Fatalities:\")\n",
    "print(top10_fatal_streets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc834b-a44b-45a0-9cde-a85892da1d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "fatal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfbd03e-3c8b-4160-9aec-9e059a6f0113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure columns are numeric\n",
    "fatal_df_gps = fatal_df[[\"GPS X\", \"GPS Y\"]].copy()\n",
    "\n",
    "# Drop rows with missing or invalid coordinates\n",
    "fatal_df_gps = fatal_df_gps.dropna(subset=[\"GPS X\", \"GPS Y\"])\n",
    "\n",
    "# Check the data\n",
    "print(fatal_df_gps.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deea8a44-031f-4e83-a173-cbbe94062848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Transformer\n",
    "\n",
    "# Initialize the transformer\n",
    "transformer = Transformer.from_crs(\"epsg:3301\", \"epsg:4326\", always_xy=True)\n",
    "\n",
    "# Apply the transformation with swapped input (Y, X) for EPSG:3301\n",
    "def convert_to_latlon(row):\n",
    "    lon, lat = transformer.transform(row[\"GPS Y\"], row[\"GPS X\"])  # Swap the axes\n",
    "    return pd.Series([lon, lat], index=[\"GPS X\", \"GPS Y\"])\n",
    "\n",
    "# Transform the coordinates\n",
    "fatal_df_gps = fatal_df.apply(convert_to_latlon, axis=1)\n",
    "\n",
    "# Round the results to 6 decimal places for clarity\n",
    "fatal_df_gps[\"GPS X\"] = fatal_df_gps[\"GPS X\"].round(6)\n",
    "fatal_df_gps[\"GPS Y\"] = fatal_df_gps[\"GPS Y\"].round(6)\n",
    "\n",
    "# Check the converted data\n",
    "print(fatal_df_gps.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3335373-b478-4221-81cd-b5ade243d3fd",
   "metadata": {},
   "source": [
    "Visualizing Streets on a Map\n",
    "We'll use the folium library to visualize the top accident locations on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bc6c80-b3fe-44ff-a531-e2e3279315d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Drop rows with NaN values, if any\n",
    "fatal_df_gps = fatal_df_gps.dropna(subset=[\"GPS X\", \"GPS Y\"])\n",
    "\n",
    "# Center the map based on the mean coordinates\n",
    "map_center_x = fatal_df_gps[\"GPS X\"].mean()\n",
    "map_center_y = fatal_df_gps[\"GPS Y\"].mean()\n",
    "\n",
    "# Create a Folium map\n",
    "m = folium.Map(location=[map_center_y, map_center_x], zoom_start=7)\n",
    "\n",
    "# Prepare data for the heatmap\n",
    "heat_data = fatal_df_gps[[\"GPS Y\", \"GPS X\"]].values.tolist()\n",
    "\n",
    "# Add the HeatMap layer\n",
    "HeatMap(heat_data).add_to(m)\n",
    "\n",
    "# Display HeatMap\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0070a8fc-7785-4a4c-abb6-65b5e1b9437a",
   "metadata": {},
   "source": [
    "# Task 3: Data Preparation\n",
    "In this section, we'll prepare the data for modeling. This includes handling missing values, encoding categorical variables, feature engineering, and splitting the data into training and testing sets\n",
    "\n",
    "## Data Cleaning and Handling Missing Values\n",
    "We'll first check for missing values and decide on strategies to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb5c412-4926-4e0d-a0de-1867f7fdc62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved date parsing function\n",
    "def parse_date(date_str):\n",
    "    date_formats = [\n",
    "        '%d.%m.%Y %H:%M',\n",
    "        '%d.%m.%Y',\n",
    "        '%d/%m/%Y %H:%M',\n",
    "        '%d/%m/%Y',\n",
    "        '%d.%m.%y %H:%M',\n",
    "        '%d.%m.%y',\n",
    "        '%d/%m/%y %H:%M',\n",
    "        '%d/%m/%y',\n",
    "        '%Y-%m-%d %H:%M:%S',\n",
    "        '%Y-%m-%d',\n",
    "    ]\n",
    "    for fmt in date_formats:\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt, dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "    return pd.NaT\n",
    "\n",
    "# Apply the improved date parser\n",
    "df['Parsed_Date'] = df['Toimumisaeg'].apply(parse_date)\n",
    "\n",
    "# Drop rows with missing 'Parsed_Date'\n",
    "df = df[df['Parsed_Date'].notna()].copy()\n",
    "\n",
    "df.drop(columns=['Aadress (PPA)'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d568af9-590a-4973-bb74-1065199ab6da",
   "metadata": {},
   "source": [
    "## 3.1 Handling Missing Values\n",
    "\n",
    "We'll clean numerical columns, handle missing numerical values, and standardize categorical columns. Additionally, we'll drop columns with excessive missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb8cd6-4e3c-4958-ac95-54d9e7d98218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean numerical columns\n",
    "numerical_columns = [\n",
    "    'Isikuid', 'Hukkunuid', 'Vigastatuid', 'Sõidukeid', \n",
    "    'Sõiduradade arv', 'Lubatud sõidukiirus (PPA)'\n",
    "]\n",
    "for col in numerical_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Fill missing numerical values\n",
    "df['Sõiduradade arv'] = df['Sõiduradade arv'].fillna(df['Sõiduradade arv'].median())\n",
    "df['Lubatud sõidukiirus (PPA)'] = df['Lubatud sõidukiirus (PPA)'].fillna(df['Lubatud sõidukiirus (PPA)'].median())\n",
    "\n",
    "# For count-based columns, fill missing with 0\n",
    "count_columns = ['Isikuid', 'Hukkunuid', 'Vigastatuid', 'Sõidukeid']\n",
    "for col in count_columns:\n",
    "    df[col] = df[col].fillna(0)\n",
    "\n",
    "# Convert to integers\n",
    "df[numerical_columns] = df[numerical_columns].astype(int)\n",
    "\n",
    "# Standardize categorical variables\n",
    "categorical_columns = [\n",
    "    'Maakond (PPA)', 'Ilmastik [1]', 'Tee tüüp [1]', \n",
    "    'Valgustus [1]', 'Tee seisund', 'Teekatte seisund [2]'\n",
    "]\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "    df[col] = df[col].fillna('puudub')\n",
    "\n",
    "# Drop columns with excessive missing data\n",
    "df.drop(columns=['Asula',\"Omavalitsus (PPA)\",'Maja nr (PPA)', 'Ristuv tänav (PPA)', 'Asustus (PPA)', 'Tee nr (PPA)', 'Tee km (PPA)', \"Tänav (PPA)\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6052b9-52e9-4913-9762-f20b20bac7b6",
   "metadata": {},
   "source": [
    "## 3.2 Encoding Categorical Variables\n",
    "\n",
    "We'll encode binary categorical variables using label encoding and other categorical variables with one-hot encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b46d76-ef83-4481-a90d-9cd4f3aa13bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding for binary categorical variables\n",
    "binary_columns = ['Jalakäija osalusel', 'Kaassõitja osalusel', 'Sõiduautojuhi osalusel','Esmase juhiloa omaniku osalusel', 'Eaka (65+) mootorsõidukijuhi osalusel', 'Alaealise osalusel']\n",
    "for col in binary_columns:\n",
    "    df[col] = df[col].map({'JAH': 1, 'jah': 1, 'EI': 0, 'ei': 0}).fillna(0).astype(int)\n",
    "\n",
    "# One-hot encoding for other categorical variables\n",
    "categorical_cols = [\n",
    "    'Maakond (PPA)', 'Ilmastik [1]', 'Tee tüüp [1]', 'Valgustus [1]', \n",
    "    'Tee seisund', 'Teekatte seisund [2]'\n",
    "]\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44921a6b-33f9-4425-b636-e832dc29926c",
   "metadata": {},
   "source": [
    "## 3.3 Feature Engineering\n",
    "\n",
    "We'll create a new feature for accident severity and time-based features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afa1d39-2ca5-4260-baad-f2a7f097893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create time-based features\n",
    "df_encoded['Is_Weekend'] = df['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "df_encoded['Is_Night'] = df['Hour'].apply(lambda x: 1 if (x >= 22 or x <= 5) else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c159579f-26bc-4605-8e26-06b866c96d64",
   "metadata": {},
   "source": [
    "## 3.4 Splitting the Data\n",
    "\n",
    "We'll split the data into training and testing sets, ensuring no target leakage by removing 'Hukkunuid' and 'Vigastatuid' columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cec387-3085-4aed-a530-86eea75aac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df_encoded.drop(columns=['Injury_Accident'], inplace=True, errors='ignore')\n",
    "\n",
    "# Define the features and target variable\n",
    "features = df_encoded.drop(['Juhtumi nr', 'Toimumisaeg', 'Parsed_Date', 'Time_Present', 'Fatal_Accident', 'Hukkunuid', 'Vigastatuid'], axis=1)\n",
    "target = df_encoded['Fatal_Accident']\n",
    "\n",
    "# Split the data<\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42, stratify=target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e2a33-d174-4765-85f6-a094895c96b2",
   "metadata": {},
   "source": [
    "# Task 4: Modeling\n",
    "\n",
    "In this task, we'll train models, evaluate them, and compare their performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addec7b5-1994-4c34-87e4-792677184a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling categorical columns for models\n",
    "categorical_cols_to_encode = [\n",
    "    'Liiklusõnnetuse liik [1]', 'Liiklusõnnetuse liik [3]', 'Tüüpskeemi nr', \n",
    "    'Tüüpskeem [2]', 'Tee tüüp [2]', 'Tee element [1]', \n",
    "    'Tee element [2]', 'Tee objekt [2]', 'Kurvilisus', \n",
    "    'Tee tasasus', 'Teekate', 'Valgustus [2]'\n",
    "]\n",
    "\n",
    "# Combine X_train and X_test for consistent encoding\n",
    "X_combined = pd.concat([X_train, X_test], axis=0)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "X_combined_encoded = pd.get_dummies(X_combined, columns=categorical_cols_to_encode)\n",
    "\n",
    "# Split back into X_train and X_test\n",
    "X_train_encoded = X_combined_encoded.iloc[:len(X_train), :].reset_index(drop=True)\n",
    "X_test_encoded = X_combined_encoded.iloc[len(X_train):, :].reset_index(drop=True)\n",
    "\n",
    "# Update X_train and X_test\n",
    "X_train = X_train_encoded\n",
    "X_test = X_test_encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3bd9c9-9d24-4cc4-80f9-27816a45f334",
   "metadata": {},
   "source": [
    "## 4.1 Handling Class Imbalance\n",
    "\n",
    "We'll use SMOTE to oversample the minority class in the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ffe114-d5cb-4841-b4ea-a54e9d316228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "numeric_cols_to_convert = [\n",
    "    'Kergliikurijuhi osalusel',\n",
    "    'Maastikusõiduki juhi osalusel',\n",
    "    'Bussijuhi osalusel',\n",
    "    'Veoautojuhi osalusel',\n",
    "    'Ühissõidukijuhi osalusel',\n",
    "    'Mootorratturi osalusel',\n",
    "    'Mopeedijuhi osalusel',\n",
    "    'Jalgratturi osalusel',\n",
    "    'Turvavarustust mitte kasutanud isiku osalusel',\n",
    "    'Mootorsõidukijuhi osalusel',\n",
    "    'GPS X',\n",
    "    'GPS Y'\n",
    "]\n",
    "\n",
    "# Copy the original ones\n",
    "X_train_original = X_train.copy()\n",
    "y_train_original = y_train.copy()\n",
    "X_test_original = X_test.copy()\n",
    "\n",
    "# Convert to numeric, handling errors\n",
    "for col in numeric_cols_to_convert:\n",
    "    X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
    "    X_test[col] = pd.to_numeric(X_test[col], errors='coerce')\n",
    "# Fill NaN values with appropriate values, e.g., 0\n",
    "for col in numeric_cols_to_convert:\n",
    "    X_train[col] = X_train[col].fillna(0)\n",
    "    X_test[col] = X_test[col].fillna(0)\n",
    "\n",
    "# Apply SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fed923-536f-4dfc-9941-85b3c942f095",
   "metadata": {},
   "source": [
    "## 4.2 Model Development\n",
    "\n",
    "We'll train and evaluate Logistic Regression, Random Forest, and Gradient Boosting models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e558f5-960f-418e-bf9f-8daa4f17b25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=3000, random_state=42)\n",
    "lr_model.fit(X_train_res, y_train_res)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "y_proba_lr = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "print(\"Logistic Regression:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_proba_lr)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759ebf5c-cafa-43e6-bedf-50d09386dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=200,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,\n",
    "    criterion=\"entropy\",\n",
    "    class_weight='balanced',\n",
    "    random_state=20\n",
    ")\n",
    "\n",
    "# Lets use the data before SMOTE, as it gets better results for random forests\n",
    "rf_model.fit(X_train_original, y_train_original)\n",
    "y_pred_rf = rf_model.predict(X_test_original)\n",
    "y_proba_rf = rf_model.predict_proba(X_test_original)[:, 1]\n",
    "\n",
    "# Evaluate Random Forest\n",
    "print(\"Random Forest:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_proba_rf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1090ecbe-a0c1-4a62-bf39-eaae11ed06fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train_res, y_train_res)\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "y_proba_gb = gb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate Gradient Boosting\n",
    "print(\"Gradient Boosting:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_proba_gb)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e26f40-9466-4da2-abc4-277c1c5b12a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# CatBoost model\n",
    "catboost_model = CatBoostClassifier(\n",
    "    eval_metric='AUC',\n",
    "    random_seed=42,\n",
    "    verbose=100,\n",
    "    early_stopping_rounds=200 \n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "cb_model = catboost_model\n",
    "cb_model.fit(X_train_res, y_train_res)\n",
    "y_pred_cb = cb_model.predict(X_test)\n",
    "y_proba_cb = cb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate CatBoost\n",
    "print(\"CatBoost:\")\n",
    "print(classification_report(y_test, y_pred_cb))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_proba_cb)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b30de1e-7eb8-4a8d-9bdd-5da4a16e6a4a",
   "metadata": {},
   "source": [
    "## 4.3 Model Comparison\n",
    "\n",
    "We'll compare the models' performance based on ROC AUC scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc509f1-a5b8-4ec9-a5c5-d8816d9541af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Function to plot the ROC curve for multiple models\n",
    "def plot_multiple_roc_curves(models, y_test, probabilities, labels):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for model, y_proba, label in zip(models, probabilities, labels):\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "        auc_score = roc_auc_score(y_test, y_proba)\n",
    "        plt.plot(fpr, tpr, label=f'{label} (AUC = {auc_score:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for All Models')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Collect model probabilities and labels\n",
    "probabilities = [y_proba_lr, y_proba_rf, y_proba_gb, y_proba_cb]\n",
    "labels = ['Logistic Regression', 'Random Forest', 'Gradient Boosting', 'CatBoost']\n",
    "\n",
    "# Plot all ROC curves\n",
    "plot_multiple_roc_curves([lr_model, rf_model, gb_model, cb_model], y_test, probabilities, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4754a902-3467-4972-804d-e8e1bc6baa35",
   "metadata": {},
   "source": [
    "## Most critical features for determing fatality in the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f992b-de7c-478a-91a4-f90596af5247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance for Logistic Regression\n",
    "lr_feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': lr_model.coef_[0]\n",
    "})\n",
    "lr_feature_importance['Abs_Coefficient'] = lr_feature_importance['Coefficient'].abs()\n",
    "lr_feature_importance = lr_feature_importance.sort_values(by='Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"Top 10 Important Features for Logistic Regression:\")\n",
    "\n",
    "print(lr_feature_importance.head(10))\n",
    "\n",
    "# Get feature importance for Random Forest\n",
    "rf_feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "})\n",
    "rf_feature_importance = rf_feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Important Features for Random Forest:\")\n",
    "print(rf_feature_importance.head(10))\n",
    "\n",
    "\n",
    "# Get feature importance for Gradient Boosting\n",
    "gb_feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': gb_model.feature_importances_\n",
    "})\n",
    "gb_feature_importance = gb_feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Important Features for Gradient Boosting:\")\n",
    "print(gb_feature_importance.head(10))\n",
    "\n",
    "# Get feature importance for CatBoost\n",
    "cb_feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': cb_model.feature_importances_\n",
    "})\n",
    "cb_feature_importance = cb_feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Important Features for CatBoost:\")\n",
    "print(cb_feature_importance.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1434f456-510e-4aa7-ae9d-abbdba9fa4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting function for feature importance\n",
    "def plot_feature_importance(df, model_name, top_n=10):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=df.head(top_n), x='Importance', y='Feature')\n",
    "    plt.title(f\"Top {top_n} Features for {model_name}\")\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for Random Forest\n",
    "plot_feature_importance(rf_feature_importance, \"Random Forest\")\n",
    "\n",
    "# Plot for Gradient Boosting\n",
    "plot_feature_importance(gb_feature_importance, \"Gradient Boosting\")\n",
    "\n",
    "# Plot for CatBoost\n",
    "plot_feature_importance(cb_feature_importance, \"CatBoost\")\n",
    "\n",
    "# For Logistic Regression\n",
    "plot_feature_importance(lr_feature_importance.rename(columns={'Abs_Coefficient': 'Importance'}), \"Logistic Regression\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
